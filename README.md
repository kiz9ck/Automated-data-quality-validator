# Automated Data Quality Validator

## ðŸ“Œ Overview
An automated ETL pipeline developed in Python to validate data quality in a simulated enterprise environment. The tool ingests raw CSV data, detects corrupt records based on business logic, and loads clean data into a SQL database for analysis.

## ðŸš€ Key Features
* **Automated Validation:** Checks for null values, duplicates, and data type mismatches.
* **Business Logic Enforcement:** Validates logic rules (e.g., salaries must be positive, email formats).
* **Error Reporting:** Separates corrupted data into a `processed/error_report.csv` for audit.
* **SQL Integration:** Stores valid records in SQLite for persistent storage.

## ðŸ›  Tech Stack
* **Python 3.x**
* **Pandas & NumPy** (Data Processing)
* **SQLAlchemy & SQLite** (Database)

## ðŸ“‚ Project Structure
```text
data_validator/
â”œâ”€â”€ data/               # Storage for raw CSVs and SQLite DB
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generator.py    # Generates dummy data with intentional errors
â”‚   â”œâ”€â”€ validator.py    # Core validation logic (Class-based)
â”‚   â”œâ”€â”€ database.py     # SQL handling
â”‚   â””â”€â”€ analysis.py     # Post-processing verification
â””â”€â”€ main.py             # Entry point for the ETL pipeline